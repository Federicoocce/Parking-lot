{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93282d19",
   "metadata": {
    "papermill": {
     "duration": 0.005238,
     "end_time": "2024-05-23T15:33:55.080292",
     "exception": false,
     "start_time": "2024-05-23T15:33:55.075054",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86187c0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T15:33:55.090494Z",
     "iopub.status.busy": "2024-05-23T15:33:55.090128Z",
     "iopub.status.idle": "2024-05-23T15:34:02.507889Z",
     "shell.execute_reply": "2024-05-23T15:34:02.506907Z"
    },
    "papermill": {
     "duration": 7.426005,
     "end_time": "2024-05-23T15:34:02.510618",
     "exception": false,
     "start_time": "2024-05-23T15:33:55.084613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessari\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import torchvision\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162fe909",
   "metadata": {
    "papermill": {
     "duration": 0.004451,
     "end_time": "2024-05-23T15:34:02.528501",
     "exception": false,
     "start_time": "2024-05-23T15:34:02.524050",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "DATASET CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adcee2a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T15:34:02.539677Z",
     "iopub.status.busy": "2024-05-23T15:34:02.538729Z",
     "iopub.status.idle": "2024-05-23T15:34:02.551926Z",
     "shell.execute_reply": "2024-05-23T15:34:02.551035Z"
    },
    "papermill": {
     "duration": 0.020753,
     "end_time": "2024-05-23T15:34:02.553878",
     "exception": false,
     "start_time": "2024-05-23T15:34:02.533125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import re\n",
    "\n",
    "class ParkingLotDataset(Dataset):\n",
    "    def __init__(self, root_img, root_msk, pairs=None, transforms=None, mask_transforms=None):\n",
    "        self.root_img = root_img\n",
    "        self.root_msk = root_msk\n",
    "        self.transforms = transforms\n",
    "        self.mask_transforms = mask_transforms\n",
    "\n",
    "        if pairs is None:\n",
    "            # Get all image files\n",
    "            self.image_paths = sorted(glob.glob(os.path.join(root_img, '*.png')))\n",
    "\n",
    "            # Get all mask files\n",
    "            self.mask_paths = sorted(glob.glob(os.path.join(root_msk, '*.png')))\n",
    "\n",
    "            # Pair image and mask files based on their filenames\n",
    "            #self.pairs = [(image_path, mask_path) for image_path in self.image_paths for mask_path in self.mask_paths if os.path.splitext(os.path.basename(image_path))[0] == os.path.splitext(os.path.basename(mask_path))[0]]\n",
    "            self.pairs = []\n",
    "\n",
    "            for image_path in self.image_paths:\n",
    "                image_filename = os.path.splitext(os.path.basename(image_path))[0]\n",
    "                \n",
    "                # Remove suffix like _1, _2, _3 using regular expressions\n",
    "                image_filename_base = re.sub(r'_[1-9]$', '', image_filename)\n",
    "                \n",
    "                mask_filename = f\"{image_filename_base}_SegmentationClass.png\"\n",
    "                mask_path = os.path.join(root_msk, mask_filename)\n",
    "                \n",
    "                if os.path.exists(mask_path):\n",
    "                    self.pairs.append((image_path, mask_path))\n",
    "\n",
    "        else:\n",
    "            self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx, threshold=0.5):\n",
    "        image_path, mask_path = self.pairs[idx]\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(image_path)\n",
    "        image_array = np.array(image)\n",
    "        self.input_channels = image_array.shape[0]\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms:\n",
    "            image_array = self.transforms(image_array)\n",
    "        \n",
    "        #mask = Image.open(mask_path)\n",
    "        #mask_array = np.array(mask)\n",
    "        mask_array = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        mask_array = (mask_array > threshold).astype(np.float32)\n",
    "        self.input_channels = mask_array.shape[0]\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.mask_transforms:\n",
    "            mask_array = self.mask_transforms(mask_array)\n",
    "\n",
    "\n",
    "        return image_array, mask_array\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc65ae4d",
   "metadata": {},
   "source": [
    "Dataset splits and data_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3909a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "import torch\n",
    "import random\n",
    "\n",
    "\n",
    "# Define synchronized transformation class\n",
    "class SynchronizedTransform:\n",
    "    def __init__(self):\n",
    "        self.color_transforms = transforms.ColorJitter(\n",
    "            brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1\n",
    "        )\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.resize = transforms.Resize(size=(512, 512))\n",
    "        self.to_pil = transforms.ToPILImage()\n",
    "\n",
    "    def __call__(self, image, mask):\n",
    "        # Convert to tensor\n",
    "        image = self.to_tensor(image)\n",
    "        mask = self.to_tensor(mask)\n",
    "\n",
    "        # Apply Resize\n",
    "        image = self.resize(image)\n",
    "        mask = self.resize(mask)\n",
    "\n",
    "        # Apply Random Horizontal Flip\n",
    "        if torch.rand(1) < 0.5:\n",
    "            image = F.hflip(image)\n",
    "            mask = F.hflip(mask)\n",
    "\n",
    "        # Apply Random Vertical Flip\n",
    "        if torch.rand(1) < 0.5:\n",
    "            image = F.vflip(image)\n",
    "            mask = F.vflip(mask)\n",
    "\n",
    "        # Apply Random Rotation\n",
    "        angle = transforms.RandomRotation.get_params([-10, 10])\n",
    "        image = F.rotate(image, angle)\n",
    "        mask = F.rotate(mask, angle, interpolation=F.InterpolationMode.NEAREST)\n",
    "\n",
    "        # Apply Random Resized Crop\n",
    "        i, j, h, w = transforms.RandomResizedCrop.get_params(image, scale=(0.8, 1.0), ratio=(3.0/4.0, 4.0/3.0))\n",
    "        image = F.resized_crop(image, i, j, h, w, size=(512, 512))\n",
    "        mask = F.resized_crop(mask, i, j, h, w, size=(512, 512), interpolation=F.InterpolationMode.NEAREST)\n",
    "\n",
    "        # Apply color transforms only to the image\n",
    "        image = self.color_transforms(image)\n",
    "\n",
    "        # Convert back to PIL\n",
    "        image = self.to_pil(image)\n",
    "        mask = self.to_pil(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "sync_transform = SynchronizedTransform()\n",
    "\n",
    "def save_image(image, path, filename):\n",
    "    image.save(os.path.join(path, filename))\n",
    "\n",
    "def augment_and_save(image_path, mask_path, save_image_path, save_mask_path, num_augmentations=3):\n",
    "    #image_files = sorted(os.listdir(image_path))\n",
    "    #mask_files = sorted(os.listdir(mask_path))\n",
    "\n",
    "    if not os.path.exists(save_image_path):\n",
    "        os.makedirs(save_image_path)\n",
    "    if not os.path.exists(save_mask_path):\n",
    "        os.makedirs(save_mask_path)\n",
    "\n",
    "    current_index = 0\n",
    "\n",
    "    \n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "    # Save original image and mask\n",
    "    save_image(img, save_image_path, f\"{current_index}.png\")\n",
    "    save_image(mask, save_mask_path, f\"{current_index}_SegmentationClass.png\")\n",
    "    #current_index += 1\n",
    "\n",
    "    for _ in range(num_augmentations):\n",
    "        augmented_img, augmented_mask = sync_transform(img, mask)\n",
    "\n",
    "        save_image(augmented_img, save_image_path, f\"{current_index}.png\")\n",
    "        save_image(augmented_mask, save_mask_path, f\"{current_index}_SegmentationClass.png\")\n",
    "        current_index += 1\n",
    "\n",
    "# Percorsi delle cartelle\n",
    "#image_path = 'Resize_no_padding'\n",
    "image_path1 = 'Resize_no_padding_1'\n",
    "image_path2 = 'C:/Users/loren/Documents/GitHub/Parking-lot/Resize_no_padding_2'\n",
    "image_path3 = 'C:/Users/loren/Documents/GitHub/Parking-lot/Resize_no_padding_3'\n",
    "mask_path = 'Resizedmasks_nopad'\n",
    "\n",
    "# Crea le cartelle di destinazione\n",
    "train_image_path = 'train_images_2'\n",
    "train_mask_path = 'train_masks'\n",
    "val_image_path = 'val_images'\n",
    "val_mask_path = 'val_masks'\n",
    "test_image_path = 'test_images'\n",
    "test_mask_path = 'test_masks'\n",
    "\n",
    "os.makedirs(train_image_path, exist_ok=True)\n",
    "os.makedirs(train_mask_path, exist_ok=True)\n",
    "os.makedirs(val_image_path, exist_ok=True)\n",
    "os.makedirs(val_mask_path, exist_ok=True)\n",
    "os.makedirs(test_image_path, exist_ok=True)\n",
    "os.makedirs(test_mask_path, exist_ok=True)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "mask_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Crea il dataset\n",
    "dataset1 = ParkingLotDataset(image_path1, mask_path, transforms=transform, mask_transforms=mask_transforms)\n",
    "dataset2 = ParkingLotDataset(image_path2, mask_path, transforms=transform, mask_transforms=mask_transforms)\n",
    "dataset3 = ParkingLotDataset(image_path3, mask_path, transforms=transform, mask_transforms=mask_transforms)\n",
    "\n",
    "# Definisci le proporzioni per la divisione\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "total_size = len(dataset1)+len(dataset2)+len(dataset3)\n",
    "print(total_size)\n",
    "\n",
    "# Mischia il dataset\n",
    "random.shuffle(dataset1.pairs)\n",
    "random.shuffle(dataset2.pairs)\n",
    "random.shuffle(dataset3.pairs)\n",
    "\n",
    "# Calcola le dimensioni di ogni split\n",
    "train_size1 = int(train_ratio * len(dataset1))\n",
    "val_size1 = int(val_ratio * len(dataset1))\n",
    "\n",
    "train_size2 = int(train_ratio * len(dataset2))\n",
    "val_size2 = int(val_ratio * len(dataset2))\n",
    "\n",
    "train_size3 = int(train_ratio * len(dataset3))\n",
    "val_size3 = int(val_ratio * len(dataset3))\n",
    "\n",
    "# Creazione del set di training\n",
    "train_pairs = dataset1.pairs[:train_size1] + \\\n",
    "              dataset2.pairs[:train_size2] + \\\n",
    "              dataset3.pairs[:train_size3]\n",
    "\n",
    "# Creazione del set di validazione\n",
    "val_pairs = dataset1.pairs[train_size1:train_size1 + val_size1] + \\\n",
    "            dataset2.pairs[train_size2:train_size2 + val_size2] + \\\n",
    "            dataset3.pairs[train_size3:train_size3 + val_size3]\n",
    "\n",
    "# Creazione del set di test\n",
    "test_pairs = dataset1.pairs[train_size1 + val_size1:] + \\\n",
    "             dataset2.pairs[train_size2 + val_size2:] + \\\n",
    "             dataset3.pairs[train_size3 + val_size3:]\n",
    "\n",
    "# Applica data augmentation e salva il train dataset\n",
    "# Applica data augmentation e salva il train dataset\n",
    "for i, (image_path, mask_path) in enumerate(tqdm(train_pairs, desc=\"Augmenting train dataset\")):\n",
    "    augment_and_save(image_path, mask_path, os.path.join(train_image_path, str(i)), os.path.join(train_mask_path, str(i)))\n",
    "\n",
    "# Salva il val dataset\n",
    "for i, (image_path, mask_path) in enumerate(tqdm(val_pairs, desc=\"Saving val dataset\")):\n",
    "    image = Image.open(image_path)\n",
    "    mask = Image.open(mask_path)\n",
    "    save_image(image, val_image_path, f\"{i}.png\")\n",
    "    save_image(mask, val_mask_path, f\"{i}_SegmentationClass.png\")\n",
    "\n",
    "# Salva il test dataset\n",
    "for i, (image_path, mask_path) in enumerate(tqdm(test_pairs, desc=\"Saving test dataset\")):\n",
    "    image = Image.open(image_path)\n",
    "    mask = Image.open(mask_path)\n",
    "    save_image(image, test_image_path, f\"{i}.png\")\n",
    "    save_image(mask, test_mask_path, f\"{i}_SegmentationClass.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ed7337",
   "metadata": {
    "papermill": {
     "duration": 0.005358,
     "end_time": "2024-05-23T15:34:02.563658",
     "exception": false,
     "start_time": "2024-05-23T15:34:02.558300",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "DATASET SLPIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0213346f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T15:34:02.574091Z",
     "iopub.status.busy": "2024-05-23T15:34:02.573771Z",
     "iopub.status.idle": "2024-05-23T15:34:02.878645Z",
     "shell.execute_reply": "2024-05-23T15:34:02.877761Z"
    },
    "papermill": {
     "duration": 0.312753,
     "end_time": "2024-05-23T15:34:02.881168",
     "exception": false,
     "start_time": "2024-05-23T15:34:02.568415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "# magari rifare il dataloader con due cartelle\n",
    "train_path = '/kaggle/input/big-ddd/Dataset_splittato/train_images'\n",
    "train_mask_path = '/kaggle/input/big-ddd/Dataset_splittato/train_masks'\n",
    "\n",
    "val_path = '/kaggle/input/big-ddd/Dataset_splittato/val_images'\n",
    "val_mask_path = '/kaggle/input/big-ddd/Dataset_splittato/val_masks'\n",
    "\n",
    "test_path = '/kaggle/input/big-ddd/Dataset_splittato/test_images'\n",
    "test_mask_path = '/kaggle/input/big-ddd/Dataset_splittato/test_masks'\n",
    "\n",
    "#normalize = transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "#normalize = transforms.Normalize(mean=[35.5, 35.2, 33.4], std=[21.8, 21.6, 20.9])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    \n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    #transforms.Normalize(mean=[35.5, 35.2, 33.4], std=[21.8, 21.6, 20.9]),\n",
    "    # Add other transforms here as needed\n",
    "])\n",
    "\n",
    "mask_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "    # Add other mask transformations here\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Create datasets for each split\n",
    "train_dataset = ParkingLotDataset(train_path, train_mask_path, transforms=transform, mask_transforms=mask_transforms)\n",
    "val_dataset = ParkingLotDataset(val_path, val_mask_path, transforms=transform, mask_transforms=mask_transforms)\n",
    "test_dataset = ParkingLotDataset(test_path, test_mask_path, transforms=transform, mask_transforms=mask_transforms)\n",
    "\n",
    "# Now you can create data loaders for each split\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fced2e8",
   "metadata": {
    "papermill": {
     "duration": 0.084043,
     "end_time": "2024-05-23T15:34:08.630878",
     "exception": false,
     "start_time": "2024-05-23T15:34:08.546835",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58968220",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T15:34:08.791486Z",
     "iopub.status.busy": "2024-05-23T15:34:08.790502Z",
     "iopub.status.idle": "2024-05-23T15:34:08.805785Z",
     "shell.execute_reply": "2024-05-23T15:34:08.804971Z"
    },
    "papermill": {
     "duration": 0.097728,
     "end_time": "2024-05-23T15:34:08.807932",
     "exception": false,
     "start_time": "2024-05-23T15:34:08.710204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SmallUNet_RGB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallUNet_RGB, self).__init__()\n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.dropout4 = nn.Dropout(0.2)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv3 = nn.Conv2d(384, 128, 3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        self.dropout5 = nn.Dropout(0.2)\n",
    "        self.upconv2 = nn.Conv2d(192, 64, 3, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(64)\n",
    "        self.dropout6 = nn.Dropout(0.2)\n",
    "        self.upconv1 = nn.Conv2d(96, 32, 3, padding=1)\n",
    "        self.bn7 = nn.BatchNorm2d(32)\n",
    "        self.dropout7 = nn.Dropout(0.2)\n",
    "        self.final_conv = nn.Conv2d(32, 1, 1)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        conv1 = self.dropout1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.maxpool(conv1)\n",
    "        conv2 = self.dropout2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.maxpool(conv2)\n",
    "        conv3 = self.dropout3(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.maxpool(conv3)\n",
    "        x = self.dropout4(F.relu(self.bn4(self.conv4(x))))\n",
    "\n",
    "        # Decoder\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "        x = self.dropout5(F.relu(self.bn5(self.upconv3(x))))\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, conv2], dim=1)\n",
    "        x = self.dropout6(F.relu(self.bn6(self.upconv2(x))))\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, conv1], dim=1)\n",
    "        x = self.dropout7(F.relu(self.bn7(self.upconv1(x))))\n",
    "        out = self.final_conv(x)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9146a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def dice_loss(pred, target, smooth=1e-6):\n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous()\n",
    "    \n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    loss = (2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)\n",
    "    \n",
    "    return 1 - loss.mean()\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, weight_dice=0.5, weight_bce=0.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.weight_dice = weight_dice\n",
    "        self.weight_bce = weight_bce\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(outputs, targets)\n",
    "        dice = dice_loss(torch.sigmoid(outputs), targets)\n",
    "        return self.weight_bce * bce_loss + self.weight_dice * dice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f8daa6",
   "metadata": {
    "papermill": {
     "duration": 0.081453,
     "end_time": "2024-05-23T15:34:08.968501",
     "exception": false,
     "start_time": "2024-05-23T15:34:08.887048",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8073e9b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T15:34:09.129855Z",
     "iopub.status.busy": "2024-05-23T15:34:09.129464Z",
     "iopub.status.idle": "2024-05-23T15:43:42.126952Z",
     "shell.execute_reply": "2024-05-23T15:43:42.125762Z"
    },
    "papermill": {
     "duration": 573.080554,
     "end_time": "2024-05-23T15:43:42.129283",
     "exception": false,
     "start_time": "2024-05-23T15:34:09.048729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch import save\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Start a new run\n",
    "#from Architetture import SmallUNet_RGB\n",
    "\n",
    "os.environ['WANDB_API_KEY'] = 'cf05b564865bb4bf8601ed59cbace5b02a587fa9'\n",
    "#wandb.login('cf05b564865bb4bf8601ed59cbace5b02a587fa9')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = SmallUNet_RGB().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "criterion = CombinedLoss()\n",
    "epochs = 100\n",
    "update_loss=1\n",
    "\n",
    "run = wandb.init(\n",
    "    #Set the project where this run will be logged\n",
    "    project=\"Parking_lot_zones\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": 1e-2,\n",
    "        \"epochs\": epochs,\n",
    "    },\n",
    "    #entity='lorenzo_barbieri'\n",
    "    entity='occelli-2127855'\n",
    ")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, masks in train_loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device, dtype=torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    wandb.log({\"Train Loss\": train_loss})\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device, dtype=torch.float32)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)    \n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    if val_loss<update_loss:\n",
    "        torch.save(model.state_dict(), '/kaggle/working/best_model_1_e2_no_labeled.pth')\n",
    "        print(\"model saved\")\n",
    "        update_loss = val_loss\n",
    "    wandb.log({\"Validation Loss\": val_loss})\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1143fe46",
   "metadata": {},
   "source": [
    "Metrics definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1f3e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def iou_score(pred, target):\n",
    "    intersection = np.logical_and(pred, target)\n",
    "    union = np.logical_or(pred, target)\n",
    "    return np.sum(intersection) / np.sum(union)\n",
    "\n",
    "def dice_coefficient(pred, target):\n",
    "    intersection = np.sum(pred * target)\n",
    "    return (2. * intersection) / (np.sum(pred) + np.sum(target))\n",
    "\n",
    "def precision_score(pred, target):\n",
    "    true_positive = np.sum(np.logical_and(pred, target))\n",
    "    predicted_positive = np.sum(pred)\n",
    "    return true_positive / predicted_positive if predicted_positive > 0 else 0\n",
    "\n",
    "def recall_score(pred, target):\n",
    "    true_positive = np.sum(np.logical_and(pred, target))\n",
    "    actual_positive = np.sum(target)\n",
    "    return true_positive / actual_positive if actual_positive > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81cdd43",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a78ac5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T15:43:42.311720Z",
     "iopub.status.busy": "2024-05-23T15:43:42.311357Z",
     "iopub.status.idle": "2024-05-23T15:43:58.831353Z",
     "shell.execute_reply": "2024-05-23T15:43:58.830155Z"
    },
    "papermill": {
     "duration": 16.614775,
     "end_time": "2024-05-23T15:43:58.833953",
     "exception": false,
     "start_time": "2024-05-23T15:43:42.219178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Create directories to save images\n",
    "save_dir = '/kaggle/working/segmentation_results_e2_no_label'\n",
    "os.makedirs(os.path.join(save_dir, 'input_images'), exist_ok=True)\n",
    "os.makedirs(os.path.join(save_dir, 'ground_truth'), exist_ok=True)\n",
    "os.makedirs(os.path.join(save_dir, 'predictions'), exist_ok=True)\n",
    "\n",
    "model = SmallUNet_RGB().to(device)\n",
    "model.load_state_dict(torch.load('/kaggle/working/best_model_1_e2_no_labeled.pth'))\n",
    "model.eval()\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "total_iou = 0\n",
    "total_dice = 0\n",
    "total_precision = 0\n",
    "total_recall = 0\n",
    "num_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "        images, masks = batch[:2]\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device, dtype=torch.float32)\n",
    "        outputs = model(images)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        \n",
    "        total += masks.numel()\n",
    "        correct += (predicted == masks).sum().item()\n",
    "        \n",
    "        # Calculate additional metrics and save images\n",
    "        for i in range(images.size(0)):\n",
    "            pred_np = predicted[i][0].cpu().numpy()\n",
    "            mask_np = masks[i][0].cpu().numpy()\n",
    "            \n",
    "            total_iou += iou_score(pred_np, mask_np)\n",
    "            total_dice += dice_coefficient(pred_np, mask_np)\n",
    "            total_precision += precision_score(pred_np, mask_np)\n",
    "            total_recall += recall_score(pred_np, mask_np)\n",
    "            num_samples += 1\n",
    "            \n",
    "            # Save input image\n",
    "            input_img = Image.fromarray((images[i].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8))\n",
    "            input_img.save(os.path.join(save_dir, 'input_images', f'input_{batch_idx}_{i}.png'))\n",
    "            \n",
    "            # Save ground truth mask\n",
    "            gt_mask = Image.fromarray((mask_np * 255).astype(np.uint8))\n",
    "            gt_mask.save(os.path.join(save_dir, 'ground_truth', f'gt_{batch_idx}_{i}.png'))\n",
    "            \n",
    "            # Save predicted mask\n",
    "            pred_mask = Image.fromarray((pred_np * 255).astype(np.uint8))\n",
    "            pred_mask.save(os.path.join(save_dir, 'predictions', f'pred_{batch_idx}_{i}.png'))\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "mean_iou = total_iou / num_samples\n",
    "mean_dice = total_dice / num_samples\n",
    "mean_precision = total_precision / num_samples\n",
    "mean_recall = total_recall / num_samples\n",
    "\n",
    "print(f\"Accuracy on test set: {accuracy:.2f}%\")\n",
    "print(f\"Mean IoU: {mean_iou:.4f}\")\n",
    "print(f\"Mean Dice Coefficient: {mean_dice:.4f}\")\n",
    "print(f\"Mean Precision: {mean_precision:.4f}\")\n",
    "print(f\"Mean Recall: {mean_recall:.4f}\")\n",
    "\n",
    "# Log metrics to wandb\n",
    "wandb.log({\n",
    "    \"Test Accuracy\": accuracy,\n",
    "    \"Mean IoU\": mean_iou,\n",
    "    \"Mean Dice Coefficient\": mean_dice,\n",
    "    \"Mean Precision\": mean_precision,\n",
    "    \"Mean Recall\": mean_recall\n",
    "})\n",
    "\n",
    "print(f\"Images saved in {save_dir}\")\n",
    "\n",
    "wandb.finish()\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "def zipdir(path, ziph):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            ziph.write(os.path.join(root, file), \n",
    "                       os.path.relpath(os.path.join(root, file), \n",
    "                                       os.path.join(path, '..')))\n",
    "\n",
    "output_dir = '/kaggle/working/segmentation_results_e2_no_label'  # La directory che vuoi scaricare\n",
    "zipf = zipfile.ZipFile('/kaggle/working/segmentation_results_no_label.zip', 'w', zipfile.ZIP_DEFLATED)\n",
    "zipdir(output_dir, zipf)\n",
    "zipf.close()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5069787,
     "sourceId": 8496340,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5070683,
     "sourceId": 8497563,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 609.510358,
   "end_time": "2024-05-23T15:44:01.725082",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-23T15:33:52.214724",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
