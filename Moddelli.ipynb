{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8442619,"sourceType":"datasetVersion","datasetId":5030007},{"sourceId":8444224,"sourceType":"datasetVersion","datasetId":5031465}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Imports","metadata":{}},{"cell_type":"code","source":"# Import necessari\nimport cv2\nimport numpy as np\nimport os\nimport pickle\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom sklearn.model_selection import train_test_split\nimport json\nimport torchvision\nfrom torchvision.models.detection import maskrcnn_resnet50_fpn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms","metadata":{"execution":{"iopub.status.busy":"2024-05-18T17:39:45.690655Z","iopub.execute_input":"2024-05-18T17:39:45.691648Z","iopub.status.idle":"2024-05-18T17:39:49.405845Z","shell.execute_reply.started":"2024-05-18T17:39:45.691599Z","shell.execute_reply":"2024-05-18T17:39:49.405052Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Dataset preparation","metadata":{}},{"cell_type":"markdown","source":"DATASET CLASS","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nfrom PIL import Image\nimport os\nimport glob\nfrom PIL import Image\n\nclass ParkingLotDataset(Dataset):\n    def __init__(self, root_img, root_msk, pairs=None, transforms=None, mask_transforms=None):\n        self.root_img = root_img\n        self.root_msk = root_msk\n        self.transforms = transforms\n        self.mask_transforms = mask_transforms\n\n        if pairs is None:\n            # Get all image files\n            self.image_paths = sorted(glob.glob(os.path.join(root_img, '*.png')))\n\n            # Get all mask files\n            self.mask_paths = sorted(glob.glob(os.path.join(root_msk, '*.png')))\n\n            # Pair image and mask files based on their filenames\n            #self.pairs = [(image_path, mask_path) for image_path in self.image_paths for mask_path in self.mask_paths if os.path.splitext(os.path.basename(image_path))[0] == os.path.splitext(os.path.basename(mask_path))[0]]\n            self.pairs = []\n\n            for image_path in self.image_paths:\n                image_filename = os.path.splitext(os.path.basename(image_path))[0]\n                mask_filename = f\"{image_filename}_SegmentationClass.png\"\n                mask_path = os.path.join(root_msk, mask_filename)\n                if os.path.exists(mask_path):\n                    self.pairs.append((image_path, mask_path))\n\n        else:\n            self.pairs = pairs\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        image_path, mask_path = self.pairs[idx]\n\n        # Load image\n        image = Image.open(image_path)\n        image_array = np.array(image)\n        self.input_channels = image_array.shape[0]\n\n        # Apply transformations\n        if self.transforms:\n            image_array = self.transforms(image_array)\n        \n        #mask = Image.open(mask_path)\n        #mask_array = np.array(mask)\n        mask_array = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n        self.input_channels = mask_array.shape[0]\n\n        # Apply transformations\n        if self.mask_transforms:\n            mask_array = self.mask_transforms(mask_array)\n\n\n        return image_array, mask_array\n        \n","metadata":{"execution":{"iopub.status.busy":"2024-05-18T17:39:49.409437Z","iopub.execute_input":"2024-05-18T17:39:49.409837Z","iopub.status.idle":"2024-05-18T17:39:49.421526Z","shell.execute_reply.started":"2024-05-18T17:39:49.409811Z","shell.execute_reply":"2024-05-18T17:39:49.420614Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"DATASET SLPIT","metadata":{}},{"cell_type":"code","source":"import random\nfrom torch.utils.data import random_split\nfrom torchvision.transforms import functional as F\n\n# magari rifare il dataloader con due cartelle\nimage_path = '/kaggle/input/rgbdataset/RGB_dataset'\nmask_path = '//kaggle/input/512-images/Resizedmasks_smaller'\n\n#normalize = transforms.Normalize(mean=[0.5], std=[0.5])\n#normalize = transforms.Normalize(mean=[35.5, 35.2, 33.4], std=[21.8, 21.6, 20.9])\n\ntransform = transforms.Compose([\n    \n    transforms.ToTensor(),\n    transforms.Normalize(mean=[35.5, 35.2, 33.4], std=[21.8, 21.6, 20.9]),\n    # Add other transforms here as needed\n])\n\nmask_transforms = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.5]),\n    # Add other mask transformations here\n])\n\n\n# Create the dataset\ndataset = ParkingLotDataset(image_path, mask_path, transforms=transform, mask_transforms=mask_transforms )\n\n# Define the proportions for the split\ntrain_ratio = 0.7\nval_ratio = 0.2\ntest_ratio = 0.1\n\ntotal_size = len(dataset)\nprint(total_size)\n\n# Shuffle the dataset\nrandom.shuffle(dataset.pairs)\n\n# Calculate the sizes of each split\ntotal_size = len(dataset)\nprint(total_size)\ntrain_size = int(train_ratio * total_size)\nval_size = int(val_ratio * total_size)\ntest_size = total_size - train_size - val_size\n\n# Split the dataset\ntrain_pairs = dataset.pairs[:train_size]\nval_pairs = dataset.pairs[train_size:train_size+val_size]\ntest_pairs = dataset.pairs[train_size+val_size:]\n\n# Create datasets for each split\ntrain_dataset = ParkingLotDataset(image_path, mask_path, pairs=train_pairs, transforms=transform)\nval_dataset = ParkingLotDataset(image_path, mask_path, pairs=val_pairs, transforms=transform)\ntest_dataset = ParkingLotDataset(image_path, mask_path, pairs=test_pairs, transforms=transform )\n\n# Now you can create data loaders for each split\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=False)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False) \n","metadata":{"execution":{"iopub.status.busy":"2024-05-18T17:39:49.422586Z","iopub.execute_input":"2024-05-18T17:39:49.422864Z","iopub.status.idle":"2024-05-18T17:39:49.701287Z","shell.execute_reply.started":"2024-05-18T17:39:49.422832Z","shell.execute_reply":"2024-05-18T17:39:49.700330Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"500\n500\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Assuming you have a DataLoader object called train_loader\nnum_samples = 10\n\n# Iterate over batches from the DataLoader\nfor batch_idx, (images, masks) in enumerate(train_loader):\n    # Get the number of samples in the current batch\n    batch_size = images.size(0)\n\n    # Iterate over the samples in the batch\n    for i in range(batch_size):\n        # Convert the image tensor to a NumPy array\n        image_array = images[i].permute(1, 2, 0).numpy()\n\n        # Convert the mask tensor to a NumPy array\n        mask_array = masks[i].permute(1, 2, 0).numpy()\n\n        # Plot the image and the combined mask\n        plt.figure(figsize=(12, 6))\n        plt.subplot(1, 2, 1)\n        plt.imshow(image_array, cmap='gray')\n        plt.title(f'Image {batch_idx * batch_size + i + 1}')\n        plt.axis('off')\n\n        plt.subplot(1, 2, 2)\n        plt.imshow(image_array, cmap='gray')\n        plt.imshow(mask_array, cmap='gray', alpha=0.5)\n        plt.title(f'Image {batch_idx * batch_size + i + 1} with Masks')\n        plt.axis('off')\n\n        plt.show()\n\n        # Break after the first 10 samples\n        if batch_idx * batch_size + i == num_samples - 1:\n            break\n\n    # Break after processing one batch\n    if batch_idx == 0:\n        break","metadata":{"execution":{"iopub.status.busy":"2024-05-18T17:42:53.354202Z","iopub.execute_input":"2024-05-18T17:42:53.354614Z","iopub.status.idle":"2024-05-18T17:42:53.520837Z","shell.execute_reply.started":"2024-05-18T17:42:53.354583Z","shell.execute_reply":"2024-05-18T17:42:53.519598Z"},"trusted":true},"execution_count":7,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Convert the mask tensor to a NumPy array\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#mask_array = masks[i].permute(1, 2, 0).numpy()\u001b[39;00m\n\u001b[1;32m     18\u001b[0m dense_mask \u001b[38;5;241m=\u001b[39m masks[i]\u001b[38;5;241m.\u001b[39mto_dense()\n\u001b[0;32m---> 19\u001b[0m mask_array \u001b[38;5;241m=\u001b[39m \u001b[43mdense_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Plot the image and the combined mask\u001b[39;00m\n\u001b[1;32m     22\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n","\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3"],"ename":"RuntimeError","evalue":"permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3","output_type":"error"}]},{"cell_type":"markdown","source":"Model","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torchvision import models\nfrom torch import nn\nimport torch.nn.functional as F\n\nclass SmallUNet(nn.Module):\n    def __init__(self):\n        super(SmallUNet, self).__init__()\n        \n        # Encoder\n        #self.conv1 = nn.Conv2d(1, 32, 3, padding=1)  #scala di grigi\n        self.conv1 = nn.Conv2d(3, 32, 3, padding=1) # Modificato il numero di canali di input a 1\n        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n        self.maxpool = nn.MaxPool2d(2)\n        \n        # Decoder\n        self.upconv3 = nn.Conv2d(384, 128, 3, padding=1)\n        self.upconv2 = nn.Conv2d(192, 64, 3, padding=1)\n        self.upconv1 = nn.Conv2d(96, 32, 3, padding=1)\n        self.final_conv = nn.Conv2d(32, 1, 1)\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n\n    def forward(self, x):\n        # Encoder\n        conv1 = F.relu(self.conv1(x))\n        x = self.maxpool(conv1)\n        \n        conv2 = F.relu(self.conv2(x))\n        x = self.maxpool(conv2)\n        \n        conv3 = F.relu(self.conv3(x))\n        x = self.maxpool(conv3)\n        \n        x = F.relu(self.conv4(x))\n        \n        # Decoder\n        x = self.upsample(x)\n        x = torch.cat([x, conv3], dim=1)\n        x = F.relu(self.upconv3(x))\n        \n        x = self.upsample(x)\n        x = torch.cat([x, conv2], dim=1)\n        x = F.relu(self.upconv2(x))\n        \n        x = self.upsample(x)\n        x = torch.cat([x, conv1], dim=1)\n        x = F.relu(self.upconv1(x))\n        \n        out = torch.sigmoid(self.final_conv(x))\n        \n        return out\n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-18T17:39:54.254203Z","iopub.execute_input":"2024-05-18T17:39:54.254577Z","iopub.status.idle":"2024-05-18T17:39:54.268749Z","shell.execute_reply.started":"2024-05-18T17:39:54.254549Z","shell.execute_reply":"2024-05-18T17:39:54.267806Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Train","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch import optim\nimport torch.nn.functional as F\nimport wandb\n\n# Start a new run\n#run = wandb.init(project='Parking_lot_zones', entity='Fede_occe')\n\n\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = SmallUNet().to(device)\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n#wandb.watch(model)\nepochs = 50\n\nfor epoch in range(epochs):\n    model.train()\n    for images, masks in train_loader:\n        images = images.to(device)\n        masks = masks.to(device)\n\n        optimizer.zero_grad()\n\n        outputs = model(images)\n        masks = masks.unsqueeze(1).type_as(outputs)  # Add a channel dimension to the masks\n        loss = F.binary_cross_entropy_with_logits(outputs, masks)\n        loss.backward()\n        optimizer.step()\n\n        #wandb.log({\"Train Loss\": loss.item()})\n\n    model.eval()\n    with torch.no_grad():\n        for images, masks in val_loader:\n            images = images.to(device)\n            masks = masks.to(device)\n\n            outputs = model(images)\n            masks = masks.unsqueeze(1).type_as(outputs)\n            val_loss = F.binary_cross_entropy_with_logits(outputs, masks)\n\n            #wandb.log({\"Validation Loss\": val_loss.item()})\n\n    print(f\"Epoch {epoch+1}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-18T17:39:57.669359Z","iopub.execute_input":"2024-05-18T17:39:57.670035Z","iopub.status.idle":"2024-05-18T17:40:52.376301Z","shell.execute_reply.started":"2024-05-18T17:39:57.670005Z","shell.execute_reply":"2024-05-18T17:40:52.375081Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: -27.742427825927734, Validation Loss: -29.41171646118164\nEpoch 2, Training Loss: -27.746206283569336, Validation Loss: -29.41555404663086\nEpoch 3, Training Loss: -27.747289657592773, Validation Loss: -29.41667938232422\nEpoch 4, Training Loss: -27.747791290283203, Validation Loss: -29.417198181152344\nEpoch 5, Training Loss: -27.74806785583496, Validation Loss: -29.417495727539062\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, masks \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m---> 22\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     masks \u001b[38;5;241m=\u001b[39m masks\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     25\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]}]}