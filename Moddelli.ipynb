{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessari\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import torchvision\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATASET CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "class ParkingLotDataset(Dataset):\n",
    "    def __init__(self, root_img, root_msk, pairs=None, transforms=None):\n",
    "        self.root_img = root_img\n",
    "        self.root_msk = root_msk\n",
    "        self.transforms = transforms\n",
    "\n",
    "        if pairs is None:\n",
    "            # Get all image files\n",
    "            self.image_paths = sorted(glob.glob(os.path.join(root_img, '*.png')))\n",
    "\n",
    "            # Get all mask files\n",
    "            self.mask_paths = sorted(glob.glob(os.path.join(root_msk, '*.png')))\n",
    "\n",
    "            # Pair image and mask files based on their filenames\n",
    "            #self.pairs = [(image_path, mask_path) for image_path in self.image_paths for mask_path in self.mask_paths if os.path.splitext(os.path.basename(image_path))[0] == os.path.splitext(os.path.basename(mask_path))[0]]\n",
    "            self.pairs = []\n",
    "\n",
    "            for image_path in self.image_paths:\n",
    "                image_filename = os.path.splitext(os.path.basename(image_path))[0]\n",
    "                mask_filename = f\"{image_filename}_SegmentationClass.png\"\n",
    "                mask_path = os.path.join(root_msk, mask_filename)\n",
    "                if os.path.exists(mask_path):\n",
    "                    self.pairs.append((image_path, mask_path))\n",
    "\n",
    "        else:\n",
    "            self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, mask_path = self.pairs[idx]\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(image_path)\n",
    "        image_array = np.array(image)\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms:\n",
    "            image_array = self.transforms(image_array)\n",
    "            print(image_array.shape)\n",
    "        \n",
    "        #mask = Image.open(mask_path)\n",
    "        #mask_array = np.array(mask)\n",
    "        mask_array = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms:\n",
    "            mask_array = self.transforms(mask_array)\n",
    "            print(mask_array.shape)\n",
    "\n",
    "\n",
    "        return image_array, mask_array\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATASET SLPIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "# magari rifare il dataloader con due cartelle\n",
    "image_path = '/kaggle/input/rgbdataset/RGB_dataset'\n",
    "mask_path = '//kaggle/input/512-images/Resizedmasks_smaller'\n",
    "\n",
    "#normalize = transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "normalize = transforms.Normalize(mean=[35.5, 35.2, 33.4], std=[21.8, 21.6, 20.9])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    \n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "    # Add other transforms here as needed\n",
    "])\n",
    "\n",
    "\n",
    "# Create the dataset\n",
    "dataset = ParkingLotDataset(image_path, mask_path, transforms=transform)\n",
    "\n",
    "# Define the proportions for the split\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "total_size = len(dataset)\n",
    "print(total_size)\n",
    "\n",
    "# Shuffle the dataset\n",
    "random.shuffle(dataset.pairs)\n",
    "\n",
    "# Calculate the sizes of each split\n",
    "total_size = len(dataset)\n",
    "print(total_size)\n",
    "train_size = int(train_ratio * total_size)\n",
    "val_size = int(val_ratio * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "# Split the dataset\n",
    "train_pairs = dataset.pairs[:train_size]\n",
    "val_pairs = dataset.pairs[train_size:train_size+val_size]\n",
    "test_pairs = dataset.pairs[train_size+val_size:]\n",
    "\n",
    "# Create datasets for each split\n",
    "train_dataset = ParkingLotDataset(image_path, mask_path, pairs=train_pairs, transforms=transform)\n",
    "val_dataset = ParkingLotDataset(image_path, mask_path, pairs=val_pairs, transforms=transform)\n",
    "test_dataset = ParkingLotDataset(image_path, mask_path, pairs=test_pairs, transforms=transform )\n",
    "\n",
    "# Now you can create data loaders for each split\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a DataLoader object called train_loader\n",
    "num_samples = 10\n",
    "\n",
    "# Iterate over batches from the DataLoader\n",
    "for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "    # Get the number of samples in the current batch\n",
    "    batch_size = images.size(0)\n",
    "\n",
    "    # Iterate over the samples in the batch\n",
    "    for i in range(batch_size):\n",
    "        # Convert the image tensor to a NumPy array\n",
    "        image_array = images[i].permute(1, 2, 0).numpy()\n",
    "\n",
    "        # Convert the mask tensor to a NumPy array\n",
    "        mask_array = masks[i].permute(1, 2, 0).numpy()\n",
    "\n",
    "        # Plot the image and the combined mask\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(image_array, cmap='gray')\n",
    "        plt.title(f'Image {batch_idx * batch_size + i + 1}')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(image_array, cmap='gray')\n",
    "        plt.imshow(mask_array, cmap='gray', alpha=0.5)\n",
    "        plt.title(f'Image {batch_idx * batch_size + i + 1} with Masks')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        # Break after the first 10 samples\n",
    "        if batch_idx * batch_size + i == num_samples - 1:\n",
    "            break\n",
    "\n",
    "    # Break after processing one batch\n",
    "    if batch_idx == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SmallUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallUNet, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        #self.conv1 = nn.Conv2d(1, 32, 3, padding=1)  #scala di grigi\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1) # Modificato il numero di canali di input a 1\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Decoder\n",
    "        self.upconv3 = nn.Conv2d(384, 128, 3, padding=1)\n",
    "        self.upconv2 = nn.Conv2d(192, 64, 3, padding=1)\n",
    "        self.upconv1 = nn.Conv2d(96, 32, 3, padding=1)\n",
    "        self.final_conv = nn.Conv2d(32, 1, 1)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        conv1 = F.relu(self.conv1(x))\n",
    "        x = self.maxpool(conv1)\n",
    "        \n",
    "        conv2 = F.relu(self.conv2(x))\n",
    "        x = self.maxpool(conv2)\n",
    "        \n",
    "        conv3 = F.relu(self.conv3(x))\n",
    "        x = self.maxpool(conv3)\n",
    "        \n",
    "        x = F.relu(self.conv4(x))\n",
    "        \n",
    "        # Decoder\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "        x = F.relu(self.upconv3(x))\n",
    "        \n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, conv2], dim=1)\n",
    "        x = F.relu(self.upconv2(x))\n",
    "        \n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, conv1], dim=1)\n",
    "        x = F.relu(self.upconv1(x))\n",
    "        \n",
    "        out = torch.sigmoid(self.final_conv(x))\n",
    "        \n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "\n",
    "# Start a new run\n",
    "#run = wandb.init(project='Parking_lot_zones', entity='Fede_occe')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = SmallUNet().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "#wandb.watch(model)\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for images, masks in train_loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = F.binary_cross_entropy_with_logits(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #wandb.log({\"Train Loss\": loss.item()})\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            val_loss = F.binary_cross_entropy_with_logits(outputs, masks)\n",
    "\n",
    "            #wandb.log({\"Validation Loss\": val_loss.item()})\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
