{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessari\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import torchvision\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATASET CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "class ParkingLotDataset(Dataset):\n",
    "    def __init__(self, root_img, root_msk, pairs=None, transforms=None, mask_transforms=None):\n",
    "        self.root_img = root_img\n",
    "        self.root_msk = root_msk\n",
    "        self.transforms = transforms\n",
    "        self.mask_transforms = mask_transforms\n",
    "\n",
    "        if pairs is None:\n",
    "            # Get all image files\n",
    "            self.image_paths = sorted(glob.glob(os.path.join(root_img, '*.png')))\n",
    "\n",
    "            # Get all mask files\n",
    "            self.mask_paths = sorted(glob.glob(os.path.join(root_msk, '*.png')))\n",
    "\n",
    "            # Pair image and mask files based on their filenames\n",
    "            #self.pairs = [(image_path, mask_path) for image_path in self.image_paths for mask_path in self.mask_paths if os.path.splitext(os.path.basename(image_path))[0] == os.path.splitext(os.path.basename(mask_path))[0]]\n",
    "            self.pairs = []\n",
    "\n",
    "            for image_path in self.image_paths:\n",
    "                image_filename = os.path.splitext(os.path.basename(image_path))[0]\n",
    "                mask_filename = f\"{image_filename}_SegmentationClass.png\"\n",
    "                mask_path = os.path.join(root_msk, mask_filename)\n",
    "                if os.path.exists(mask_path):\n",
    "                    self.pairs.append((image_path, mask_path))\n",
    "\n",
    "        else:\n",
    "            self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx, threshold=0.5):\n",
    "        image_path, mask_path = self.pairs[idx]\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(image_path)\n",
    "        image_array = np.array(image)\n",
    "        self.input_channels = image_array.shape[0]\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms:\n",
    "            image_array = self.transforms(image_array)\n",
    "        \n",
    "        #mask = Image.open(mask_path)\n",
    "        #mask_array = np.array(mask)\n",
    "        mask_array = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        mask_array = (mask_array > threshold).astype(np.float32)\n",
    "        self.input_channels = mask_array.shape[0]\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.mask_transforms:\n",
    "            mask_array = self.mask_transforms(mask_array)\n",
    "\n",
    "\n",
    "        return image_array, mask_array\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATASET SLPIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "# magari rifare il dataloader con due cartelle\n",
    "image_path = '/kaggle/input/rgbdataset/RGB_dataset'\n",
    "mask_path = '//kaggle/input/512-images/Resizedmasks_smaller'\n",
    "\n",
    "#normalize = transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "#normalize = transforms.Normalize(mean=[35.5, 35.2, 33.4], std=[21.8, 21.6, 20.9])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    \n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    #transforms.Normalize(mean=[35.5, 35.2, 33.4], std=[21.8, 21.6, 20.9]),\n",
    "    # Add other transforms here as needed\n",
    "])\n",
    "\n",
    "mask_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "    # Add other mask transformations here\n",
    "])\n",
    "\n",
    "\n",
    "# Create the dataset\n",
    "dataset = ParkingLotDataset(image_path, mask_path, transforms=transform, mask_transforms=mask_transforms)\n",
    "\n",
    "# Define the proportions for the split\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "total_size = len(dataset)\n",
    "print(total_size)\n",
    "\n",
    "# Shuffle the dataset\n",
    "random.shuffle(dataset.pairs)\n",
    "\n",
    "# Calculate the sizes of each split\n",
    "total_size = len(dataset)\n",
    "print(total_size)\n",
    "train_size = int(train_ratio * total_size)\n",
    "val_size = int(val_ratio * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "# Split the dataset\n",
    "train_pairs = dataset.pairs[:train_size]\n",
    "val_pairs = dataset.pairs[train_size:train_size+val_size]\n",
    "test_pairs = dataset.pairs[train_size+val_size:]\n",
    "\n",
    "# Create datasets for each split\n",
    "train_dataset = ParkingLotDataset(image_path, mask_path, transforms=transform, mask_transforms=mask_transforms)\n",
    "val_dataset = ParkingLotDataset(image_path, mask_path, transforms=transform, mask_transforms=mask_transforms)\n",
    "test_dataset = ParkingLotDataset(image_path, mask_path, transforms=transform, mask_transforms=mask_transforms)\n",
    "\n",
    "# Now you can create data loaders for each split\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a DataLoader object called train_loader\n",
    "num_samples = 10\n",
    "\n",
    "# Iterate over batches from the DataLoader\n",
    "for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "    # Get the number of samples in the current batch\n",
    "    batch_size = images.size(0)\n",
    "\n",
    "    # Iterate over the samples in the batch\n",
    "    for i in range(batch_size):\n",
    "        # Convert the image tensor to a NumPy array\n",
    "        image_array = images[i].permute(1, 2, 0).numpy()\n",
    "\n",
    "        # Convert the mask tensor to a NumPy array\n",
    "        mask_array = masks[i].numpy()\n",
    "\n",
    "        # Plot the image and the combined mask\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(image_array)\n",
    "        plt.title(f'Image {batch_idx * batch_size + i + 1}')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(image_array)\n",
    "        plt.imshow(mask_array[0], cmap='gray', alpha=0.5)\n",
    "        plt.title(f'Image {batch_idx * batch_size + i + 1} with Masks')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        # Break after the first 10 samples\n",
    "        if batch_idx * batch_size + i == num_samples - 1:\n",
    "            break\n",
    "\n",
    "    # Break after processing one batch\n",
    "    if batch_idx == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Assuming images are stored in a directory named 'images'\n",
    "image_dir = 'images'\n",
    "\n",
    "# Iterate over all files in the image directory\n",
    "for filename in os.listdir(image_dir):\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(image_dir, filename)\n",
    "\n",
    "    # Open the image file\n",
    "    image = Image.open(file_path)\n",
    "\n",
    "    # Check if the image has more than 3 channels\n",
    "    if len(image.split()) > 3:\n",
    "        # Print the filename\n",
    "        print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SmallUNet_RGB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallUNet_RGB, self).__init__()\n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv3 = nn.Conv2d(384, 128, 3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        self.upconv2 = nn.Conv2d(192, 64, 3, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(64)\n",
    "        self.upconv1 = nn.Conv2d(96, 32, 3, padding=1)\n",
    "        self.bn7 = nn.BatchNorm2d(32)\n",
    "        self.final_conv = nn.Conv2d(32, 1, 1)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        conv1 = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(conv1)\n",
    "        conv2 = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.maxpool(conv2)\n",
    "        conv3 = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.maxpool(conv3)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "\n",
    "        # Decoder\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "        x = F.relu(self.bn5(self.upconv3(x)))\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, conv2], dim=1)\n",
    "        x = F.relu(self.bn6(self.upconv2(x)))\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, conv1], dim=1)\n",
    "        x = F.relu(self.bn7(self.upconv1(x)))\n",
    "        out = self.final_conv(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch import save\n",
    "\n",
    "# Start a new run\n",
    "#from Architetture import SmallUNet_RGB\n",
    "\n",
    "\n",
    "#wandb.login('cf05b564865bb4bf8601ed59cbace5b02a587fa9')\n",
    "#os.environ['WANDB_API_KEY'] = 'cf05b564865bb4bf8601ed59cbace5b02a587fa9'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = SmallUNet_RGB().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=20, gamma=0.1) #after 20 epochs the lr becomes 0.001 and after 40 0.0001\n",
    "\n",
    "epochs = 51\n",
    "\n",
    "#run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    #project=\"Parking_lot_zones\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    #config={\n",
    "        #\"learning_rate\": 0.01,\n",
    "        #\"epochs\": epochs,\n",
    "    #},\n",
    "    #entity='lorenzo_barbieri'\n",
    "    #entity='occelli-2127855'\n",
    "#)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for images, masks in train_loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device, dtype=torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        pos_weight = torch.tensor([masks.sum() / (1 - masks).sum()]).to(device)\n",
    "        loss = F.binary_cross_entropy_with_logits(outputs, masks, pos_weight=pos_weight)\n",
    "        #loss = F.binary_cross_entropy(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #wandb.log({\"Train Loss\": loss.item()})\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device, dtype=torch.float32)\n",
    "\n",
    "            outputs = model(images)\n",
    "            val_loss = F.binary_cross_entropy_with_logits(outputs, masks)\n",
    "            #val_loss = F.binary_cross_entropy(outputs, masks)\n",
    "\n",
    "            #wandb.log({\"Validation Loss\": val_loss.item()})\n",
    "        if epoch == 19:\n",
    "            save(model.state_dict(), 'saved_model_20_epoch.pth')\n",
    "            print(\"Model saved after 20 epochs.\")\n",
    "        if epoch == 49:\n",
    "            save(model.state_dict(), 'saved_model_50_epoch.pth')\n",
    "            print(\"Model saved after 50 epochs.\")\n",
    "            \n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the saved model\n",
    "model = SmallUNet_RGB()\n",
    "model.load_state_dict(torch.load('saved_model_20_epoch.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Testing loop\n",
    "total = 0\n",
    "correct = 0\n",
    "batch_idx = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        images, masks = batch[:2]  # Unpack the first two values (images and masks)\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device, dtype=torch.float32)\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Threshold the outputs to obtain binary masks\n",
    "        predicted = (outputs > 0.5).float()\n",
    "\n",
    "        # Compute the accuracy\n",
    "        total += masks.numel()  # Total number of elements in the batch\n",
    "        correct += (predicted == masks).sum().item()  # Correctly predicted elements\n",
    "\n",
    "        # Plot the first 5 outputs\n",
    "        if batch_idx < 5:\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.imshow(images[0].permute(1, 2, 0).cpu().numpy())\n",
    "            plt.title('Input Image')\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.imshow(masks[0][0].cpu(), cmap='gray')\n",
    "            plt.title('Ground Truth Mask')\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.imshow(predicted[0][0].cpu(), cmap='gray')\n",
    "            plt.title('Predicted Mask')\n",
    "            plt.show()\n",
    "        batch_idx += 1\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Accuracy on test set: {accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
