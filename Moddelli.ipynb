{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessari\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import torchvision\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#controllo il massimo valore di altezza e larghezza per fare il padding\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Directory containing the images\n",
    "data_dir = 'Dataset'\n",
    "\n",
    "# Initialize variables to store max height and width\n",
    "max_height = 0\n",
    "max_width = 0\n",
    "\n",
    "# Iterate through all images in the directory\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith('.png') or filename.endswith('.jpg'):\n",
    "        # Load the image\n",
    "        image_path = os.path.join(data_dir, filename)\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # Get height and width of the image\n",
    "        height, width, _ = image.shape\n",
    "        if(height > 1000 or width > 1000):\n",
    "            print(filename)\n",
    "            print(image.shape)\n",
    "\n",
    "        # Update max_height and max_width if necessary\n",
    "        if height > max_height:\n",
    "            max_height = height\n",
    "        if width > max_width:\n",
    "            max_width = width\n",
    "\n",
    "        \n",
    "\n",
    "#Image with different max_height and max_width: Queretaro-TEC_parcheggi.png\n",
    "print(\"Maximum Height:\", max_height)\n",
    "print(\"Maximum Width:\", max_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store canny edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'Dataset'\n",
    "edge_path = 'Dataset_edge'\n",
    "def get_edge_image(image, low_threshold, high_threshold):\n",
    "    # Convert the image to grayscale if it's not already\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Apply Canny edge detection\n",
    "    edges = cv2.Canny(gray, low_threshold, high_threshold)\n",
    "    \n",
    "    return edges\n",
    "#edge detection + padding immagini\n",
    "for filename in os.listdir(dataset_path):\n",
    "    if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "        \n",
    "        input_image_path = os.path.join(dataset_path, filename)\n",
    "        image = cv2.imread(input_image_path)\n",
    "        # Ottieni l'immagine del bordo\n",
    "        edge_image = get_edge_image(image, low_threshold=250, high_threshold=350)\n",
    "        \n",
    "        # Apply padding to the edge image\n",
    "        max_height = 2108\n",
    "        max_width = 2113\n",
    "        padding_height = max(0, max_height - edge_image.shape[0])\n",
    "        padding_width = max(0, max_width - edge_image.shape[1])\n",
    "        edge_image_padded = cv2.copyMakeBorder(edge_image, 0, padding_height, 0, padding_width, cv2.BORDER_CONSTANT, value=0)\n",
    "        # Verificare se il nome del file contiene \"_testing\" o \"_training\"\n",
    "        if \"_testing\" in filename or \"_training\" in filename:\n",
    "            # Rinominare il file rimuovendo \"_testing\" o \"_training\" dal nome\n",
    "            new_filename = filename.replace(\"_testing\", \"\").replace(\"_training\", \"\")\n",
    "            \n",
    "            print(f\"Rinominato il file {filename} in {new_filename}\")\n",
    "            # Salva l'immagine del bordo nella cartella di output\n",
    "            output_image_path = os.path.join(edge_path, new_filename)\n",
    "            cv2.imwrite(output_image_path, edge_image_padded)\n",
    "\n",
    "print(\"Il rilevamento dei bordi Ã¨ stato applicato a tutte le immagini nella cartella.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the paths to the mask and image folders\n",
    "mask_folder = 'Dataset_mask'\n",
    "image_folder = 'Dataset_edge'\n",
    "\n",
    "# Create a new folder to store the combined images and masks\n",
    "combined_folder = 'Dataset_combined'\n",
    "os.makedirs(combined_folder, exist_ok=True)\n",
    "\n",
    "# Copy all files from the mask folder to the combined folder\n",
    "for mask_file in os.listdir(mask_folder):\n",
    "    shutil.copy2(os.path.join(mask_folder, mask_file), combined_folder)\n",
    "\n",
    "# Copy all files from the image folder to the combined folder\n",
    "for image_file in os.listdir(image_folder):\n",
    "    shutil.copy2(os.path.join(image_folder, image_file), combined_folder)\n",
    "\n",
    "print(\"Masks and images have been combined and stored in the 'Combined' folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATASET CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "class ParkingLotDataset(Dataset):\n",
    "    def __init__(self, root_img, root_msk, pairs=None, transforms=None):\n",
    "        self.root_img = root_img\n",
    "        self.root_msk = root_msk\n",
    "        self.transforms = transforms\n",
    "\n",
    "        if pairs is None:\n",
    "            # Get all image files\n",
    "            self.image_paths = sorted(glob.glob(os.path.join(root_img, '*.png')))\n",
    "\n",
    "            # Get all mask files\n",
    "            self.mask_paths = sorted(glob.glob(os.path.join(root_msk, '*.png')))\n",
    "\n",
    "            # Pair image and mask files based on their filenames\n",
    "            #self.pairs = [(image_path, mask_path) for image_path in self.image_paths for mask_path in self.mask_paths if os.path.splitext(os.path.basename(image_path))[0] == os.path.splitext(os.path.basename(mask_path))[0]]\n",
    "            self.pairs = []\n",
    "\n",
    "            for image_path in self.image_paths:\n",
    "                image_filename = os.path.splitext(os.path.basename(image_path))[0]\n",
    "                for mask_path in self.mask_paths:\n",
    "                    mask_filename = os.path.splitext(os.path.basename(mask_path))[0]\n",
    "                    if image_filename in mask_filename:\n",
    "                        self.pairs.append((image_path, mask_path))\n",
    "                        break  # Se trova una corrispondenza, passa al prossimo file immagine\n",
    "        else:\n",
    "            self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, mask_path = self.pairs[idx]\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(image_path)\n",
    "        image_array = np.array(image)\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms:\n",
    "            image_array = self.transforms(image_array)\n",
    "            print(image_array.shape)\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "        return image_array, masks\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATASET SLPIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#controllo il massimo valore di altezza e larghezza per fare il padding\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Directory containing the images\n",
    "data_dir = 'Dataset_combined'\n",
    "\n",
    "# Initialize variables to store max height and width\n",
    "max_height = 0\n",
    "max_width = 0\n",
    "\n",
    "# Iterate through all images in the directory\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith('.png') or filename.endswith('.jpg'):\n",
    "        # Load the image\n",
    "        image_path = os.path.join(data_dir, filename)\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # Get height and width of the image\n",
    "        height, width, _ = image.shape\n",
    "\n",
    "        # Update max_height and max_width if necessary\n",
    "        if height > max_height:\n",
    "            max_height = height\n",
    "        if width > max_width:\n",
    "            max_width = width\n",
    "\n",
    "        # Check if the image has different max_height and max_width\n",
    "        if height != 2108 or width != 2113:\n",
    "            print(\"Image with different max_height and max_width:\", filename)\n",
    "\n",
    "#Image with different max_height and max_width: Queretaro-TEC_parcheggi.png\n",
    "print(\"Maximum Height:\", max_height)\n",
    "print(\"Maximum Width:\", max_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "# magari rifare il dataloader con due cartelle\n",
    "image_path = 'Dataset_combined'\n",
    "\n",
    "target_size = (max_width, max_height)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    \n",
    "    transforms.ToTensor(),\n",
    "    # Add other transforms here as needed\n",
    "])\n",
    "\n",
    "\n",
    "# Create the dataset\n",
    "dataset = ParkingLotDataset(image_path, transforms=transform)\n",
    "\n",
    "# Define the proportions for the split\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "\n",
    "# Shuffle the dataset\n",
    "random.shuffle(dataset.pairs)\n",
    "\n",
    "# Calculate the sizes of each split\n",
    "total_size = len(dataset)\n",
    "print(total_size)\n",
    "train_size = int(train_ratio * total_size)\n",
    "val_size = int(val_ratio * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "# Split the dataset\n",
    "train_pairs = dataset.pairs[:train_size]\n",
    "val_pairs = dataset.pairs[train_size:train_size+val_size]\n",
    "test_pairs = dataset.pairs[train_size+val_size:]\n",
    "\n",
    "# Create datasets for each split\n",
    "train_dataset = ParkingLotDataset(image_path, pairs=train_pairs, transforms=transform)\n",
    "val_dataset = ParkingLotDataset(image_path, pairs=val_pairs, transforms=transform)\n",
    "test_dataset = ParkingLotDataset(image_path, pairs=test_pairs, transforms=transform )\n",
    "\n",
    "# Now you can create data loaders for each split\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models.segmentation import fcn_resnet50\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Define the model\n",
    "model = fcn_resnet50(pretrained=False, num_classes=2).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for images, masks in train_loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)['out']\n",
    "        loss = criterion(outputs, masks)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print loss for every epoch\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for images, targets in train_data_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_data_loader:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            # You can add here code to compute metrics and print them\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
