{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessari\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import torchvision\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATASET CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "class ParkingLotDataset(Dataset):\n",
    "    def __init__(self, root_img, root_msk, pairs=None, transforms=None, mask_transforms=None):\n",
    "        self.root_img = root_img\n",
    "        self.root_msk = root_msk\n",
    "        self.transforms = transforms\n",
    "        self.mask_transforms = mask_transforms\n",
    "\n",
    "        if pairs is None:\n",
    "            # Get all image files\n",
    "            self.image_paths = sorted(glob.glob(os.path.join(root_img, '*.png')))\n",
    "\n",
    "            # Get all mask files\n",
    "            self.mask_paths = sorted(glob.glob(os.path.join(root_msk, '*.png')))\n",
    "\n",
    "            # Pair image and mask files based on their filenames\n",
    "            #self.pairs = [(image_path, mask_path) for image_path in self.image_paths for mask_path in self.mask_paths if os.path.splitext(os.path.basename(image_path))[0] == os.path.splitext(os.path.basename(mask_path))[0]]\n",
    "            self.pairs = []\n",
    "\n",
    "            for image_path in self.image_paths:\n",
    "                image_filename = os.path.splitext(os.path.basename(image_path))[0]\n",
    "                mask_filename = f\"{image_filename}_SegmentationClass.png\"\n",
    "                mask_path = os.path.join(root_msk, mask_filename)\n",
    "                if os.path.exists(mask_path):\n",
    "                    self.pairs.append((image_path, mask_path))\n",
    "\n",
    "        else:\n",
    "            self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx, threshold=0.5):\n",
    "        image_path, mask_path = self.pairs[idx]\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(image_path)\n",
    "        image_array = np.array(image)\n",
    "        self.input_channels = image_array.shape[0]\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms:\n",
    "            image_array = self.transforms(image_array)\n",
    "        \n",
    "        #mask = Image.open(mask_path)\n",
    "        #mask_array = np.array(mask)\n",
    "        mask_array = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        mask_array = (mask_array > threshold).astype(np.float32)\n",
    "        self.input_channels = mask_array.shape[0]\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.mask_transforms:\n",
    "            mask_array = self.mask_transforms(mask_array)\n",
    "\n",
    "\n",
    "        return image_array, mask_array\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATASET SLPIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "# magari rifare il dataloader con due cartelle\n",
    "image_path = '/kaggle/input/rgbdataset/RGB_dataset'\n",
    "mask_path = '//kaggle/input/512-images/Resizedmasks_smaller'\n",
    "\n",
    "#normalize = transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "#normalize = transforms.Normalize(mean=[35.5, 35.2, 33.4], std=[21.8, 21.6, 20.9])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.12028961, 0.12028961, 0.12028961], std=[0.27084008, 0.2638215,  0.25238606]),\n",
    "    #transforms.Normalize(mean=[35.5, 35.2, 33.4], std=[21.8, 21.6, 20.9]),\n",
    "    # Add other transforms here as needed\n",
    "])\n",
    "\n",
    "mask_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "    # Add other mask transformations here\n",
    "])\n",
    "\n",
    "\n",
    "# Create the dataset\n",
    "dataset = ParkingLotDataset(image_path, mask_path, transforms=transform, mask_transforms=mask_transforms)\n",
    "\n",
    "# Define the proportions for the split\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "total_size = len(dataset)\n",
    "print(total_size)\n",
    "\n",
    "# Shuffle the dataset\n",
    "random.shuffle(dataset.pairs)\n",
    "\n",
    "# Calculate the sizes of each split\n",
    "total_size = len(dataset)\n",
    "print(total_size)\n",
    "train_size = int(train_ratio * total_size)\n",
    "val_size = int(val_ratio * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "# Split the dataset\n",
    "train_pairs = dataset.pairs[:train_size]\n",
    "val_pairs = dataset.pairs[train_size:train_size+val_size]\n",
    "test_pairs = dataset.pairs[train_size+val_size:]\n",
    "\n",
    "# Create datasets for each split\n",
    "train_dataset = ParkingLotDataset(image_path, mask_path, transforms=transform, mask_transforms=mask_transforms)\n",
    "val_dataset = ParkingLotDataset(image_path, mask_path, transforms=transform, mask_transforms=mask_transforms)\n",
    "test_dataset = ParkingLotDataset(image_path, mask_path, transforms=transform, mask_transforms=mask_transforms)\n",
    "\n",
    "# Now you can create data loaders for each split\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a DataLoader object called train_loader\n",
    "num_samples = 10\n",
    "\n",
    "# Iterate over batches from the DataLoader\n",
    "for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "    # Get the number of samples in the current batch\n",
    "    batch_size = images.size(0)\n",
    "\n",
    "    # Iterate over the samples in the batch\n",
    "    for i in range(batch_size):\n",
    "        # Convert the image tensor to a NumPy array\n",
    "        image_array = images[i].permute(1, 2, 0).numpy()\n",
    "\n",
    "        # Convert the mask tensor to a NumPy array\n",
    "        mask_array = masks[i].numpy()\n",
    "\n",
    "        # Plot the image and the combined mask\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(image_array)\n",
    "        plt.title(f'Image {batch_idx * batch_size + i + 1}')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(image_array)\n",
    "        plt.imshow(mask_array[0], cmap='gray', alpha=0.5)\n",
    "        plt.title(f'Image {batch_idx * batch_size + i + 1} with Masks')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        # Break after the first 10 samples\n",
    "        if batch_idx * batch_size + i == num_samples - 1:\n",
    "            break\n",
    "\n",
    "    # Break after processing one batch\n",
    "    if batch_idx == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallUNet_RGB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallUNet_RGB, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1) \n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Decoder\n",
    "        self.upconv3 = nn.Conv2d(384, 128, 3, padding=1)\n",
    "        self.upconv2 = nn.Conv2d(192, 64, 3, padding=1)\n",
    "        self.upconv1 = nn.Conv2d(96, 32, 3, padding=1)\n",
    "        self.final_conv = nn.Conv2d(32, 2, 1)  # Changed the number of output channels to 2\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        conv1 = F.relu(self.conv1(x))\n",
    "        x = self.maxpool(conv1)\n",
    "        \n",
    "        conv2 = F.relu(self.conv2(x))\n",
    "        x = self.maxpool(conv2)\n",
    "        \n",
    "        conv3 = F.relu(self.conv3(x))\n",
    "        x = self.maxpool(conv3)\n",
    "        \n",
    "        x = F.relu(self.conv4(x))\n",
    "        \n",
    "        # Decoder\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "        x = F.relu(self.upconv3(x))\n",
    "        \n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, conv2], dim=1)\n",
    "        x = F.relu(self.upconv2(x))\n",
    "        \n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, conv1], dim=1)\n",
    "        x = F.relu(self.upconv1(x))\n",
    "        \n",
    "        out = torch.sigmoid(self.final_conv(x))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train&Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Start a new run\n",
    "#from Architetture import SmallUNet_RGB\n",
    "\n",
    "\n",
    "#wandb.login('cf05b564865bb4bf8601ed59cbace5b02a587fa9')\n",
    "os.environ['WANDB_API_KEY'] = 'cf05b564865bb4bf8601ed59cbace5b02a587fa9'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = SmallUNet_RGB().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1) #after 10 epochs the lr becomes 0.001 and after 40 0.0001\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"Parking_lot_zones\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"epochs\": epochs,\n",
    "        \"loss_function\": \"BCEWithLogitsLoss\",\n",
    "    },\n",
    "    #entity='lorenzo_barbieri'\n",
    "    entity='occelli-2127855'\n",
    ")\n",
    "# Calculate the weights\n",
    "weights = torch.tensor([0.1, 0.9]).to(device) # 0.1 for class 0, 0.9 for class 1\n",
    "#Use a weighted loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "# Log the model\n",
    "wandb.watch(model, log='all')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for images, masks in train_loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device, dtype=torch.float32)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        #loss = F.binary_cross_entropy_with_logits(outputs, masks)\n",
    "        loss = loss_fn(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        wandb.log({\"Train Loss\": loss.item()})\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device, dtype=torch.float32)\n",
    "\n",
    "            outputs = model(images)\n",
    "            #val_loss = F.binary_cross_entropy_with_logits(outputs, masks)\n",
    "            val_loss = loss_fn(outputs, masks)\n",
    "\n",
    "            wandb.log({\"Validation Loss\": val_loss.item()})\n",
    "            \n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Testing loop\n",
    "model.eval()\n",
    "total = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for i, (images, masks) in enumerate(test_loader):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device, dtype=torch.float32)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += masks.nelement()  # total elements in the batch\n",
    "        correct += (predicted == masks).sum().item()  # correctly predicted elements\n",
    "\n",
    "        # Plot the first 5 outputs\n",
    "        if i < 5:\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.imshow(masks[0][0].cpu(), cmap='gray')  # Select the first channel\n",
    "            plt.title('Mask')\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.imshow(predicted[0].cpu(), cmap='gray', alpha=0.5)\n",
    "            plt.title('Output')\n",
    "            plt.show()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Accuracy on test set: {accuracy}%\")\n",
    "\n",
    "wandb.log({\"Test Accuracy\": accuracy})\n",
    "\n",
    "# End the wandb run\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
