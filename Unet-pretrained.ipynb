{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ed7e803",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T13:40:12.813502Z",
     "iopub.status.busy": "2024-07-19T13:40:12.813056Z",
     "iopub.status.idle": "2024-07-19T13:40:20.072210Z",
     "shell.execute_reply": "2024-07-19T13:40:20.071152Z"
    },
    "papermill": {
     "duration": 7.267448,
     "end_time": "2024-07-19T13:40:20.074989",
     "exception": false,
     "start_time": "2024-07-19T13:40:12.807541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessari\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import torchvision\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d9e4ea4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T13:40:20.083842Z",
     "iopub.status.busy": "2024-07-19T13:40:20.083016Z",
     "iopub.status.idle": "2024-07-19T13:40:20.096864Z",
     "shell.execute_reply": "2024-07-19T13:40:20.095868Z"
    },
    "papermill": {
     "duration": 0.020481,
     "end_time": "2024-07-19T13:40:20.099227",
     "exception": false,
     "start_time": "2024-07-19T13:40:20.078746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "class ParkingLotDataset(Dataset):\n",
    "    def __init__(self, root_img, root_msk, pairs=None, transforms=None, mask_transforms=None):\n",
    "        self.root_img = root_img\n",
    "        self.root_msk = root_msk\n",
    "        self.transforms = transforms\n",
    "        self.mask_transforms = mask_transforms\n",
    "\n",
    "        if pairs is None:\n",
    "            # Get all image files\n",
    "            self.image_paths = sorted(glob.glob(os.path.join(root_img, '*.png')))\n",
    "\n",
    "            # Get all mask files\n",
    "            self.mask_paths = sorted(glob.glob(os.path.join(root_msk, '*.png')))\n",
    "\n",
    "            # Pair image and mask files based on their filenames\n",
    "            #self.pairs = [(image_path, mask_path) for image_path in self.image_paths for mask_path in self.mask_paths if os.path.splitext(os.path.basename(image_path))[0] == os.path.splitext(os.path.basename(mask_path))[0]]\n",
    "            self.pairs = []\n",
    "\n",
    "            for image_path in self.image_paths:\n",
    "                image_filename = os.path.splitext(os.path.basename(image_path))[0]\n",
    "                mask_filename = f\"{image_filename}_SegmentationClass.png\"\n",
    "                mask_path = os.path.join(root_msk, mask_filename)\n",
    "                if os.path.exists(mask_path):\n",
    "                    self.pairs.append((image_path, mask_path))\n",
    "\n",
    "        else:\n",
    "            self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx, threshold=0.5):\n",
    "        image_path, mask_path = self.pairs[idx]\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(image_path)\n",
    "        image_array = np.array(image)\n",
    "        self.input_channels = image_array.shape[0]\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms:\n",
    "            image_array = self.transforms(image_array)\n",
    "        \n",
    "        #mask = Image.open(mask_path)\n",
    "        #mask_array = np.array(mask)\n",
    "        mask_array = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        mask_array = (mask_array > threshold).astype(np.float32)\n",
    "        self.input_channels = mask_array.shape[0]\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.mask_transforms:\n",
    "            mask_array = self.mask_transforms(mask_array)\n",
    "\n",
    "\n",
    "        return image_array, mask_array\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "274373a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T13:40:20.107625Z",
     "iopub.status.busy": "2024-07-19T13:40:20.106920Z",
     "iopub.status.idle": "2024-07-19T13:40:20.659942Z",
     "shell.execute_reply": "2024-07-19T13:40:20.658999Z"
    },
    "papermill": {
     "duration": 0.559925,
     "end_time": "2024-07-19T13:40:20.662571",
     "exception": false,
     "start_time": "2024-07-19T13:40:20.102646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "# magari rifare il dataloader con due cartelle\n",
    "train_path = '/kaggle/input/d-organ/train_images_2'\n",
    "train_mask_path = '/kaggle/input/d-organ/train_masks'\n",
    "\n",
    "val_path = '/kaggle/input/d-organ/val_images'\n",
    "val_mask_path = '/kaggle/input/d-organ/val_masks'\n",
    "\n",
    "test_path = '/kaggle/input/d-organ/test_images'\n",
    "test_mask_path = '/kaggle/input/d-organ/test_masks'\n",
    "\n",
    "#normalize = transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "#normalize = transforms.Normalize(mean=[35.5, 35.2, 33.4], std=[21.8, 21.6, 20.9])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    \n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    #transforms.Normalize(mean=[35.5, 35.2, 33.4], std=[21.8, 21.6, 20.9]),\n",
    "    # Add other transforms here as needed\n",
    "])\n",
    "\n",
    "mask_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "    # Add other mask transformations here\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Create datasets for each split\n",
    "train_dataset = ParkingLotDataset(train_path, train_mask_path, transforms=transform, mask_transforms=mask_transforms)\n",
    "val_dataset = ParkingLotDataset(val_path, val_mask_path, transforms=transform, mask_transforms=mask_transforms)\n",
    "test_dataset = ParkingLotDataset(test_path, test_mask_path, transforms=transform, mask_transforms=mask_transforms)\n",
    "\n",
    "# Now you can create data loaders for each split\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ab12bd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T13:40:20.670916Z",
     "iopub.status.busy": "2024-07-19T13:40:20.670454Z",
     "iopub.status.idle": "2024-07-19T14:04:39.567288Z",
     "shell.execute_reply": "2024-07-19T14:04:39.566206Z"
    },
    "papermill": {
     "duration": 1458.914028,
     "end_time": "2024-07-19T14:04:39.580154",
     "exception": false,
     "start_time": "2024-07-19T13:40:20.666126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting segmentation_models_pytorch\r\n",
      "  Downloading segmentation_models_pytorch-0.3.3-py3-none-any.whl.metadata (30 kB)\r\n",
      "Requirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.16.2)\r\n",
      "Collecting pretrainedmodels==0.7.4 (from segmentation_models_pytorch)\r\n",
      "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hCollecting efficientnet-pytorch==0.7.1 (from segmentation_models_pytorch)\r\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting timm==0.9.2 (from segmentation_models_pytorch)\r\n",
      "  Downloading timm-0.9.2-py3-none-any.whl.metadata (68 kB)\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m68.5/68.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (4.66.4)\r\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (9.5.0)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.2)\r\n",
      "Collecting munch (from pretrainedmodels==0.7.4->segmentation_models_pytorch)\r\n",
      "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (6.0.1)\r\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (0.23.2)\r\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (0.4.3)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.26.4)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (2.32.3)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.12.1)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2024.3.1)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (21.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2024.2.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (3.1.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.3.0)\r\n",
      "Downloading segmentation_models_pytorch-0.3.3-py3-none-any.whl (106 kB)\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading timm-0.9.2-py3-none-any.whl (2.2 MB)\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\r\n",
      "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\r\n",
      "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16428 sha256=0f003d3c6b0ec3f8e4a1419dc0cc207622e864701c59114d2e40d2ab3b557270\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\r\n",
      "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=427cb191f60db1aa4dbdc5aa15101539346128f71da3c47df81712185924bd75\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\r\n",
      "Successfully built efficientnet-pytorch pretrainedmodels\r\n",
      "Installing collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation_models_pytorch\r\n",
      "  Attempting uninstall: timm\r\n",
      "    Found existing installation: timm 1.0.3\r\n",
      "    Uninstalling timm-1.0.3:\r\n",
      "      Successfully uninstalled timm-1.0.3\r\n",
      "Successfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation_models_pytorch-0.3.3 timm-0.9.2\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 83.3M/83.3M [00:01<00:00, 62.6MB/s]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33moccelli-2127855\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.4 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240719_134050-77efrg5d\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbrisk-firebrand-74\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/occelli-2127855/Parking_lot_zones\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/occelli-2127855/Parking_lot_zones/runs/77efrg5d\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.4898, Validation Loss: 0.4292\n",
      "Epoch 2, Training Loss: 0.3071, Validation Loss: 0.3344\n",
      "Epoch 3, Training Loss: 0.2473, Validation Loss: 0.2801\n",
      "Epoch 4, Training Loss: 0.2183, Validation Loss: 0.2710\n",
      "Epoch 5, Training Loss: 0.1857, Validation Loss: 0.2645\n",
      "Epoch 6, Training Loss: 0.1620, Validation Loss: 0.2722\n",
      "Epoch 7, Training Loss: 0.1459, Validation Loss: 0.2449\n",
      "Epoch 8, Training Loss: 0.1366, Validation Loss: 0.2458\n",
      "Epoch 9, Training Loss: 0.1342, Validation Loss: 0.2380\n",
      "Epoch 10, Training Loss: 0.1361, Validation Loss: 0.2239\n",
      "Epoch 11, Training Loss: 0.1157, Validation Loss: 0.2682\n",
      "Epoch 12, Training Loss: 0.1233, Validation Loss: 0.2559\n",
      "Epoch 13, Training Loss: 0.1225, Validation Loss: 0.2462\n",
      "Epoch 14, Training Loss: 0.1163, Validation Loss: 0.2288\n",
      "Epoch 15, Training Loss: 0.1172, Validation Loss: 0.2321\n",
      "Epoch 16, Training Loss: 0.1120, Validation Loss: 0.2404\n",
      "Epoch 17, Training Loss: 0.1088, Validation Loss: 0.2231\n",
      "Epoch 18, Training Loss: 0.1011, Validation Loss: 0.2493\n",
      "Epoch 19, Training Loss: 0.1040, Validation Loss: 0.2384\n",
      "Epoch 20, Training Loss: 0.1083, Validation Loss: 0.2445\n",
      "Epoch 21, Training Loss: 0.0916, Validation Loss: 0.2672\n",
      "Epoch 22, Training Loss: 0.0983, Validation Loss: 0.2904\n",
      "Epoch 23, Training Loss: 0.0970, Validation Loss: 0.2237\n",
      "Epoch 24, Training Loss: 0.0814, Validation Loss: 0.2328\n",
      "Epoch 25, Training Loss: 0.0848, Validation Loss: 0.2591\n",
      "Epoch 26, Training Loss: 0.0867, Validation Loss: 0.2555\n",
      "Epoch 27, Training Loss: 0.0847, Validation Loss: 0.2664\n",
      "Epoch 28, Training Loss: 0.0815, Validation Loss: 0.2560\n",
      "Epoch 29, Training Loss: 0.0838, Validation Loss: 0.2610\n",
      "Epoch 30, Training Loss: 0.0742, Validation Loss: 0.2594\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch import save\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "!pip install segmentation_models_pytorch\n",
    "import segmentation_models_pytorch as smp\n",
    "wandb.login(key='cf05b564865bb4bf8601ed59cbace5b02a587fa9')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Load the pretrained U-Net model\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",  # Choose the encoder (backbone)\n",
    "    encoder_weights=\"imagenet\",  # Use pre-trained weights from ImageNet\n",
    "    in_channels=3,  # Input channels (RGB)\n",
    "    classes=1,  # Binary segmentation\n",
    ")\n",
    "\n",
    "# Move the model to the device\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "epochs = 30\n",
    "run = wandb.init(\n",
    "    #Set the project where this run will be logged\n",
    "    project=\"Parking_lot_zones\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"epochs\": epochs,\n",
    "    },\n",
    "    #entity='lorenzo_barbieri'\n",
    "    entity='occelli-2127855'\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, masks in train_loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device, dtype=torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    wandb.log({\"Train Loss\": train_loss})\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device, dtype=torch.float32)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    wandb.log({\"Validation Loss\": val_loss})\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08b2c02c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T14:04:39.604675Z",
     "iopub.status.busy": "2024-07-19T14:04:39.603957Z",
     "iopub.status.idle": "2024-07-19T14:04:39.613166Z",
     "shell.execute_reply": "2024-07-19T14:04:39.612092Z"
    },
    "papermill": {
     "duration": 0.024259,
     "end_time": "2024-07-19T14:04:39.615518",
     "exception": false,
     "start_time": "2024-07-19T14:04:39.591259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def iou_score(pred, target):\n",
    "    intersection = np.logical_and(pred, target)\n",
    "    union = np.logical_or(pred, target)\n",
    "    return np.sum(intersection) / np.sum(union)\n",
    "\n",
    "def dice_coefficient(pred, target):\n",
    "    intersection = np.sum(pred * target)\n",
    "    return (2. * intersection) / (np.sum(pred) + np.sum(target))\n",
    "\n",
    "def precision_score(pred, target):\n",
    "    true_positive = np.sum(np.logical_and(pred, target))\n",
    "    predicted_positive = np.sum(pred)\n",
    "    return true_positive / predicted_positive if predicted_positive > 0 else 0\n",
    "\n",
    "def recall_score(pred, target):\n",
    "    true_positive = np.sum(np.logical_and(pred, target))\n",
    "    actual_positive = np.sum(target)\n",
    "    return true_positive / actual_positive if actual_positive > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51f5527e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T14:04:39.641085Z",
     "iopub.status.busy": "2024-07-19T14:04:39.640409Z",
     "iopub.status.idle": "2024-07-19T14:04:54.982247Z",
     "shell.execute_reply": "2024-07-19T14:04:54.981204Z"
    },
    "papermill": {
     "duration": 15.35747,
     "end_time": "2024-07-19T14:04:54.984944",
     "exception": false,
     "start_time": "2024-07-19T14:04:39.627474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 92.77%\n",
      "Mean IoU: 0.7626\n",
      "Mean Dice Coefficient: 0.8588\n",
      "Mean Precision: 0.8888\n",
      "Mean Recall: 0.8399\n",
      "Images saved in /kaggle/working/segmentation_results_pretrained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Mean Dice Coefficient ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              Mean IoU ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        Mean Precision ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           Mean Recall ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         Test Accuracy ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       Validation Loss ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Mean Dice Coefficient 0.85876\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              Mean IoU 0.76255\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        Mean Precision 0.88878\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           Mean Recall 0.83986\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         Test Accuracy 92.76769\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            Train Loss 0.07425\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       Validation Loss 0.25943\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mbrisk-firebrand-74\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/occelli-2127855/Parking_lot_zones/runs/77efrg5d\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/occelli-2127855/Parking_lot_zones\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240719_134050-77efrg5d/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Create directories to save images\n",
    "save_dir = '/kaggle/working/segmentation_results_pretrained'\n",
    "os.makedirs(os.path.join(save_dir, 'input_images'), exist_ok=True)\n",
    "os.makedirs(os.path.join(save_dir, 'ground_truth'), exist_ok=True)\n",
    "os.makedirs(os.path.join(save_dir, 'predictions'), exist_ok=True)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "total_iou = 0\n",
    "total_dice = 0\n",
    "total_precision = 0\n",
    "total_recall = 0\n",
    "num_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "        images, masks = batch[:2]\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device, dtype=torch.float32)\n",
    "        outputs = model(images)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        \n",
    "        total += masks.numel()\n",
    "        correct += (predicted == masks).sum().item()\n",
    "        \n",
    "        # Calculate additional metrics and save images\n",
    "        for i in range(images.size(0)):\n",
    "            pred_np = predicted[i][0].cpu().numpy()\n",
    "            mask_np = masks[i][0].cpu().numpy()\n",
    "            \n",
    "            total_iou += iou_score(pred_np, mask_np)\n",
    "            total_dice += dice_coefficient(pred_np, mask_np)\n",
    "            total_precision += precision_score(pred_np, mask_np)\n",
    "            total_recall += recall_score(pred_np, mask_np)\n",
    "            num_samples += 1\n",
    "            \n",
    "            # Save input image\n",
    "            input_img = Image.fromarray((images[i].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8))\n",
    "            input_img.save(os.path.join(save_dir, 'input_images', f'input_{batch_idx}_{i}.png'))\n",
    "            \n",
    "            # Save ground truth mask\n",
    "            gt_mask = Image.fromarray((mask_np * 255).astype(np.uint8))\n",
    "            gt_mask.save(os.path.join(save_dir, 'ground_truth', f'gt_{batch_idx}_{i}.png'))\n",
    "            \n",
    "            # Save predicted mask\n",
    "            pred_mask = Image.fromarray((pred_np * 255).astype(np.uint8))\n",
    "            pred_mask.save(os.path.join(save_dir, 'predictions', f'pred_{batch_idx}_{i}.png'))\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "mean_iou = total_iou / num_samples\n",
    "mean_dice = total_dice / num_samples\n",
    "mean_precision = total_precision / num_samples\n",
    "mean_recall = total_recall / num_samples\n",
    "\n",
    "print(f\"Accuracy on test set: {accuracy:.2f}%\")\n",
    "print(f\"Mean IoU: {mean_iou:.4f}\")\n",
    "print(f\"Mean Dice Coefficient: {mean_dice:.4f}\")\n",
    "print(f\"Mean Precision: {mean_precision:.4f}\")\n",
    "print(f\"Mean Recall: {mean_recall:.4f}\")\n",
    "\n",
    "# Log metrics to wandb\n",
    "wandb.log({\n",
    "    \"Test Accuracy\": accuracy,\n",
    "    \"Mean IoU\": mean_iou,\n",
    "    \"Mean Dice Coefficient\": mean_dice,\n",
    "    \"Mean Precision\": mean_precision,\n",
    "    \"Mean Recall\": mean_recall\n",
    "})\n",
    "\n",
    "print(f\"Images saved in {save_dir}\")\n",
    "\n",
    "wandb.finish()\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "def zipdir(path, ziph):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            ziph.write(os.path.join(root, file), \n",
    "                       os.path.relpath(os.path.join(root, file), \n",
    "                                       os.path.join(path, '..')))\n",
    "\n",
    "output_dir = '/kaggle/working/segmentation_results_e2_no_label'  # La directory che vuoi scaricare\n",
    "zipf = zipfile.ZipFile('/kaggle/working/segmentation_results_no_label.zip', 'w', zipfile.ZIP_DEFLATED)\n",
    "zipdir(output_dir, zipf)\n",
    "zipf.close()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5246502,
     "sourceId": 8738916,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1488.020589,
   "end_time": "2024-07-19T14:04:57.720202",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-19T13:40:09.699613",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
